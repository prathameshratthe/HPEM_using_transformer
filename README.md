# Basic Human Pose Estimation in Large Video Streams for Sports Analytics 🏃‍♂️🎯

## 🔍 Project Overview

This project focuses on **real-time human pose estimation** and **action recognition** from large video streams, specifically tailored for the **sports domain**. It uses transformer-based models (e.g., **TimeSformer**) to extract keypoints and detect physical actions like running, jumping, and walking from sports footage. The aim is to support player performance analysis, movement classification, and future integration with intelligent sports analytics platforms.

---

## 🧠 Core Features

- 🎥 Efficient **frame sampling** and preprocessing from long video streams.
- 🧍 Accurate **human keypoint detection** using transformer models.
- 🏃‍♀️ Real-time **pose-based action recognition** (e.g., running, walking, jumping).
- 📊 Support for sports analysis, player movement classification, and performance tracking.
- 🔐 (Optional) **Privacy-preserving** video processing using anonymization techniques.

---

## 🛠️ Technologies Used

- **Python 3.10+**
- **PyTorch** – for model development and training
- **OpenCV** – for video reading and frame extraction
- **TimeSformer / VideoMAE** – transformer-based video understanding models
- **Pandas & NumPy** – for data handling and preprocessing
- **Matplotlib / Seaborn** – for visualizations (optional)

---

🤝 Contributors
👨🏻‍💻 Prathamesh Ratthe 
👩🏻‍💻 Feuna Khan 

📄 License
This project is licensed under the MIT License – see the LICENSE file for details.

🙌 Acknowledgments
UCF101 Dataset
MCG-NJU for the VideoMAE model
Researchers of TimeSformer, HRNet, and BlazePose
